# -*- coding: utf-8 -*-
"""Medical RAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZmSPPtM_trX2-vlnjCoVhjZH7St1WfFy

# Preprocessing Data - Building the Graph

This part aims to create the graph from the SNOMED CT Database.
"""

# Install Dependencies
# !pip install neo4j

# Imports and Drive Connect
import pandas as pd
from neo4j import GraphDatabase
from pathlib import Path
# from google.colab import drive

# Mount the drive (Only for Colab)
# drive.mount('/content/drive')


# Get the paths for the required data
# nodes = Path("/content/drive/MyDrive/MedicalRAG/Nodes.txt")
# description = Path("/content/drive/MyDrive/MedicalRAG/Description.txt")
# relationships = Path("/content/drive/MyDrive/MedicalRAG/Relationships.txt")

# Get the local paths
nodes = Path("./MedicalRAG/Nodes.txt")
description = Path("./MedicalRAG/Description.txt")
relationships = Path("./MedicalRAG/Relationships.txt")

# Read the required data
def fixDescription():
  nodes = pd.read_csv(nodes, sep='\t', dtype=str)
  relationships = pd.read_csv(relationships, sep='\t', dtype=str)

  with open(description, 'r', encoding='utf-8') as file:
        content = file.read().replace(',', '')
        
  with open(description, 'w', encoding='utf-8') as file:
        file.write(content)

  description = pd.read_csv(description, sep='\t', dtype=str)
  description.to_csv("description.csv", index=False)

# nodes.to_csv("nodes.csv", index=False)
# description.to_csv("description.csv", index=False)
# relationships.to_csv("relationship.csv", index=False)

# Neo4J Information
uri = "bolt://localhost:7687"
username = "neo4j"
password = "MedicalRAG"

# Connect to Neo4J
driver = GraphDatabase.driver(uri, auth=(username, password))

def create_indexes(transaction):
    transaction.run("CREATE INDEX FOR (c:Concept) ON (c.id);")
    
    transaction.run("CREATE INDEX FOR (r:Relationship) ON (r.sourceId);")
    transaction.run("CREATE INDEX FOR (r:Relationship) ON (r.destinationId);")
    
    transaction.run("CREATE INDEX FOR (r:Relationship) ON (r.typeId);")

def populate_nodes(transaction, nodes, descriptions, batch_size=1000):
    
  for i in range(0, len(descriptions), batch_size):
    batch = descriptions.iloc[i:i + batch_size]
    print(f"Processing batch {i // batch_size + 1} of {len(descriptions) // batch_size + 1} Descriptions.")
    
    # Create Description nodes and link them to Concept nodes in one step using UNWIND
    query = """
      UNWIND $batch AS row
      MERGE (d:Description {descriptionId: row.id})
      SET d.term = row.term, 
          d.languageCode = row.languageCode, 
          d.typeId = row.typeId, 
          d.caseSignificanceId = row.caseSignificanceId
      
      WITH d, row
      MERGE (c:Concept {id: row.conceptId})
      MERGE (c)-[:HAS_DESCRIPTION]->(d)
    """

    transaction.run(query, batch=batch.to_dict('records'))
    print(f"Batch {i // batch_size + 1}: Created and linked {len(batch)} Description nodes.")

def populate_descriptions(transaction, descriptions, batch_size=100):
    for i in range(0, len(descriptions), batch_size):
        batch = descriptions.iloc[i:i + batch_size]
        print(f"Processing batch {i//batch_size + 1} of {len(descriptions)//batch_size + 1} descriptions")  # Print batch info
        query = """
            MATCH (source:Concept {id: $sourceId}), (target:Concept {id: $destinationId})
            CREATE (source)-[:RELATIONSHIP {typeId: $typeId}]->(target)
            """
        for _, row in batch.iterrows():
            transaction.run(query, 
                            sourceId=row['conceptId'], 
                            destinationId=row['conceptId'],
                            typeId=row['typeId']) 


def populate_relationships(transaction, relationships, batch_size=500):
    for i in range(0, len(relationships), batch_size):
        batch = relationships.iloc[i:i + batch_size]
        print(f"Processing batch {i//batch_size + 1} of {len(relationships)//batch_size + 1} relationships")
        
        query = """
            UNWIND $batch AS row
            MATCH (source:Concept {id: row.sourceId}), (destination:Concept {id: row.destinationId})
            CREATE (source)-[:RELATIONSHIP {
                typeId: row.typeId,
                relationshipGroup: row.relationshipGroup,
                characteristicTypeId: row.characteristicTypeId,
                modifierId: row.modifierId
            }]->(destination)
            """
        
        # Pass the entire batch as a list of dictionaries to $batch
        transaction.run(query, batch=batch.to_dict('records'))



# Let's execute the population calls
# with driver.session() as session:
  # session.execute_write(create_indexes)
  # session.execute_write(populate_nodes, nodes, description)
  # session.execute_write(populate_descriptions, description)
  # session.execute_write(populate_relationships, relationships)

relationship_group_mapping = {
    0: "ATTRIBUTE",
    1: 'IS_A',
    2: 'PART_OF',
    3: 'ASSOCIATED_WITH',
    4: 'CAUSES',
    5: 'FOUND_AT',
    6: "TEMPORAL"
}

def relation_update():
  # Load the relationship CSV file
  relationships_df = pd.read_csv('relationship.csv')
  print(relationships_df.columns)


  # Map the 'relationshipGroup' column to the readable relationship type
  relationships_df['relationshipType'] = relationships_df['relationshipGroup'].map(relationship_group_mapping)
  columns_to_drop = [
      "id",
      "effectiveTime",
      "active",
      "moduleId",
      "relationshipGroup",
      "typeId",
      "characteristicTypeId",
      "modifierId"
  ]

  relationships_df.drop(columns=columns_to_drop, inplace=True)


  # Save the updated CSV with the new relationshipType column
  relationships_df.to_csv('modified_relationship.csv', index=False)

import csv

def clean_csv(input_path, output_path):
    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        for row in reader:
            if len(row) == 0 or any(cell.strip() == "" for cell in row):
                continue
            writer.writerow(row)
        print("Clean-up complete.")

def combine_nodes():
  nodes = pd.read_csv("nodes.csv", usecols=["id", "active"], dtype={"id": "int"})
  descriptions = pd.read_csv("description.csv", usecols=["conceptId", "term", "effectiveTime"])

  descriptions = descriptions.dropna(subset=["conceptId"])
  descriptions["conceptId"] = descriptions["conceptId"].astype(int)

  descriptions["effectiveTime"] = pd.to_numeric(descriptions["effectiveTime"], errors='coerce')
  descriptions["effectiveTime"] = descriptions["effectiveTime"].fillna(0).astype(int)

  # Merge nodes with descriptions
  merged = pd.merge(nodes, descriptions, left_on="id", right_on="conceptId", how="left")

  # Drop conceptId column
  merged.drop(columns=["conceptId"], inplace=True)

  # Group by id and active, then concatenate term
  merged = merged.groupby(["id", "active"])["term"].apply(lambda x: ';'.join(x.astype(str))).reset_index()

  # Rename term column
  merged = merged.rename(columns={"term": "descriptions"})

  merged.to_csv("merged_nodes.csv", index=False)